---
title: "Work Doc - Palma"
author: "Palma"
date: "11/16/2021"
output: html_document
---


For the projects, I want to give some suggested due dates:

Thursday 12/2: 1. Complete most of the analyses and have a draft of the "final product". 2. Begin creating a presentation. 3. Create a detailed outline of what will go in the "behind-the-scenes". 

Tuesday 12/7: 1. "Final product" complete. 2. Finish the 15 minute presentation and practice it with your group. 

Thursday 12/9: 1. Submit "final product" on moodle. 2. Give presentations - we may need to use a bit of the lunch hour. 

Thursday 12/16: Submit "behind-the-scenes" (you can always submit early, if you'd like). 


```{r}
library(nflfastR)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggrepel)
library(stringr)
library(lubridate)

library(tidymodels)        # for modeling
library(themis)            # for step functions for unbalanced data
library(doParallel)        # for parallel processing
library(stacks)            # for stacking models
library(naniar)            # for examining missing values (NAs)
library(moderndive)        # for King County housing data
library(vip)               # for variable importance plots
library(patchwork)         # for combining plots nicely
library(ranger)
library(xgboost)
library(dplyr)
```


```{r}
pbp_2018_2021 <- load_pbp(2018:2021)
nfl_qbr_weekly <- readr::read_csv("https://raw.githubusercontent.com/nflverse/espnscrapeR-data/master/data/qbr-nfl-weekly.csv")
nfl_qbr_weekly<-nfl_qbr_weekly %>% 
  filter(season==2018:2021)
```


```{r}
#devtools::install_github(repo = "ryurko/nflscrapR")
```


```{r}
library(nflscrapR)
library(na.tools)

```

```{r}
Two_min_drill <- pbp_2018_2021 %>% 
  filter(half_seconds_remaining<120, as.numeric(ms(drive_game_clock_start))<150) 
  
Two_min_drill %>% 
  group_by(game_id, drive) %>%
  select(game_id, drive_play_count, ydsnet, drive_game_clock_start, fixed_drive_result, name, posteam, week, drive_start_yard_line) %>%
  filter(game_id == "2021_01_ARI_TEN", drive==11) %>% 
  mutate(td = ifelse(fixed_drive_result=='Touchdown', 1, 0),
  fg = ifelse(fixed_drive_result=='Field goal', 1, 0)) %>%
  #mutate(yrds_to_go_start = ifelse(str_sub(drive_start_yard_line, 1, (str_locate(drive_start_yard_line, " ")-1))== posteam, 100-as.numeric(str_sub(drive_start_yard_line, (str_locate(drive_start_yard_line, " ")+1), length(drive_start_yard_line))), as.numeric(str_sub(drive_start_yard_line, (str_locate(drive_start_yard_line, " ")+1), length(drive_start_yard_line))))) %>% 
  mutate(score= ifelse(td+fg==1, 1,0)) %>% 
  right_join(nfl_qbr_weekly, by = c("week" = "game_week", "posteam"="team_abb")) %>% 
  select(!game_id.y)
  # ggplot(aes(x= score, y=qbr_total))+
  # geom_boxplot()+
  # facet_wrap(vars(score))
```


```{r}
Two_min_drill

  
```


```{r}
two_min_drill <- pbp_2018_2021 %>% 
  filter(half_seconds_remaining<120, as.numeric(ms(drive_game_clock_start))<150) 
  
two_min_new <- two_min_drill %>% 
  group_by(game_id, drive) %>%
  mutate(td = ifelse(fixed_drive_result=='Touchdown', 1, 0),
  fg = ifelse(fixed_drive_result=='Field goal', 1, 0), 
  score= ifelse(td+fg==1, 1,0)) %>%
  right_join(nfl_qbr_weekly, by = c("week" = "game_week", "posteam"="team_abb")) %>% 
  ungroup()

```

```{r}
getmode <- function(v) {
    v <- na.rm()
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
```


```{r}
two_min_new <- two_min_new[!is.na(two_min_new$passer), ] 
```

```{r}
two_min_new %>%
  select(passer, ydsnet, qbr_total, qbr_raw, game_id.x)
```


```{r}
two_min_test <- 
  two_min_new  %>%
    group_by(drive, game_id.x) %>%
    mutate(run_plays = sum(rush_attempt, na.rm = TRUE), 
           pass_plays = sum(pass_attempt, na.rm = TRUE), 
           pass_tot_yds = sum(air_yards, na.rm = TRUE), 
           completion_perc = (1- sum(incomplete_pass, na.rm = TRUE) / pass_plays), 
           tot_yds = sum(yards_gained, na.rm = TRUE),
           rush_yds_tot = sum(rushing_yards, na.rm = TRUE)) %>% 
     mutate(td = ifelse(fixed_drive_result=='Touchdown', 1, 0),
            fg = ifelse(fixed_drive_result=='Field goal', 1, 0), 
            score = ifelse(td+fg==1, 1,0)) %>%
    select(passer, qbr_raw, qbr_total, pass_tot_yds, tot_yds, ydsnet, rush_yds_tot, completion_perc, run_plays, pass_plays, drive_yards_penalized, tot_yds, drive_game_clock_start, td, fg, score, posteam, drive_start_yard_line)
```


```{r}
two_min_by_drive <-
  two_min_test %>%
    mutate(yards_to_go_start= ifelse(str_extract(drive_start_yard_line, "[A-Z]+")== posteam, 100- parse_number(drive_start_yard_line), parse_number(drive_start_yard_line))) 
  

two_min_by_drive
```

```{r}
drive_summary_data <- two_min_by_drive %>%
 arrange(game_id.x, drive) %>% 
 group_by(game_id.x) %>% 
  mutate(
    td = as.factor(td), 
    fg = as.factor(fg), 
    score = as.factor(score),
    drive_game_clock_start = as.numeric(ms(drive_game_clock_start))
    ) %>%
 summarise_all(last)

drive_summary_data
```

```{r}
drive_summary_data$yards_to_go_start[is.na(drive_summary_data$yards_to_go_start)] <- 50

```



```{r}
drive_summary_data %>%
  select(game_id.x, passer, drive_game_clock_start, completion_perc, yards_to_go_start)
```

* need third dataset

* best of drives and total yards



 -- logistic regression as primary -- predicting PERCENT OF SCORE
 
 -- more lasso / rf / xg boost -- predicting SCORE using full model -- stacking? 
 
 -- sep rmd for visualizations and modeling
 
 * time left in half / game 
 
 * Model = predictive 
 
  -- two min drill w 40 sec w all these plays, what is likelihood of scoring????
  




## Exploratory Work


1.  Exploring Data

**Intro Visualizations**


```{r}
# qbr // score

drive_summary_data %>%  
  ggplot(aes(x = qbr_total, y = score)) +
  geom_boxplot()

drive_summary_data %>%  
  ggplot(aes(x = qbr_total)) +
  geom_histogram(bins = 40) +
  geom_vline(data = filter(drive_summary_data, score=="0"), aes(xintercept= mean(qbr_total)), colour="blue") +
  geom_vline(data = filter(drive_summary_data, score=="1"), aes(xintercept= mean(qbr_total)), colour="blue") +
  facet_wrap(~score) 

drive_summary_data %>%  
  ggplot(aes(x = qbr_total)) +
  geom_freqpoly(bins = 40) + 
  geom_vline(data = filter(drive_summary_data, score=="0"), aes(xintercept= mean(qbr_total)), colour="blue") +
  geom_vline(data = filter(drive_summary_data, score=="1"), aes(xintercept= mean(qbr_total)), colour="blue") +
  facet_wrap(~score) 


```



```{r}
# qbr // compl perc

drive_summary_data %>%
   ggplot(aes(x = completion_perc)) +
   geom_histogram(bins = 15)

 drive_summary_data %>%
   ggplot(aes(x = qbr_total, y = completion_perc)) +
   geom_point()

 drive_summary_data %>%
   ggplot(aes(x = qbr_total, y = completion_perc)) +
   geom_smooth()

q <- seq(0.1, 0.9, by = 0.1)

drive_summary_data %>%  
  ggplot(aes(x = qbr_total, y = completion_perc)) +
  geom_quantile(quantiles = q)
```



```{r}
# tot yds in drive (x) // amt of time left (ms drive gamenstart from ben code) (y) , score as color or facet

# game time var = drive_game_clock_start

drive_summary_data %>%
   ggplot(aes(x = tot_yds)) +
   geom_histogram(bins = 30)

drive_summary_data %>%
   ggplot(aes(x = drive_game_clock_start)) +
   geom_histogram(bins = 30)

 drive_summary_data %>%
   ggplot(aes(y = tot_yds, x = drive_game_clock_start)) +
   geom_point()

 drive_summary_data %>%
   ggplot(aes(x = qbr_total, y = completion_perc)) +
   geom_smooth()

q <- seq(0.1, 0.9, by = 0.1)

drive_summary_data %>%  
  ggplot(aes(x = qbr_total, y = completion_perc)) +
  geom_quantile(quantiles = q)


```



**Quant Vars**

```{r}
drive_summary_data %>% 
  select(where(is.numeric)) %>% 
  pivot_longer(cols = everything(),
               names_to = "variable", 
               values_to = "value") %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30) +
  facet_wrap(vars(variable), 
             scales = "free", 
            nrow = 3)

```


# Lasso Logisitic Regression 

```{r}
library(dplyr)       # for data manipulation (dplyr) 
library(broom)       # for making model summary tidy
library(visreg)      # for plotting logodds and probability 
library(margins)     # to calculate Average Marginal Effects
library(ROCR)  
```

```{r}
drive_summary_data$completion_perc[is.na(drive_summary_data$completion_perc)] <- 0
```


```{r}
drive_summary_data <- 
  drive_summary_data %>%
  select( -posteam, -td, -fg, -game_id.x, -drive, -drive_start_yard_line)

```


```{r}

set.seed(2)

drive_two_min_split <- initial_split(drive_summary_data, 
                             prop = .75, strata = score)

drive_two_min_training <- training(drive_two_min_split)
drive_two_min_testing <- testing(drive_two_min_split)


```


3. Set up the recipe and the pre-processing steps to build a lasso model



```{r}
set.seed(2)

lasso_recipe <- recipe(score ~ ., 
                       data = drive_two_min_training) %>% 
  step_upsample(score, over_ratio = 1) %>%
  step_dummy(all_nominal(), 
             -all_outcomes()) %>%
  step_normalize(all_predictors(), 
                 -all_outcomes())

```


```{r}
lasso_recipe %>% 
  prep(drive_two_min_training) %>%
  juice() 
```




```{r}
table(drive_two_min_training$score, useNA = "always")
```


```{r}
library(glmnet)
```


# Lasso Model and WF

```{r}
lasso_mod  <- 
  logistic_reg(mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_args(penalty = tune()) %>% 
  set_mode("classification")

lasso_wf <-  workflow() %>% 
  add_recipe(lasso_recipe) %>% 
  add_model(lasso_mod)

```

# More Lasso Set up

```{r}

set.seed(2) 
cv_split <- vfold_cv(drive_two_min_training, 
                              v = 5)
ctrl <- control_resamples(save_pred = TRUE)



penalty_grid <- grid_regular(penalty(),
                             levels = 10)

lasso_tune <-  
  lasso_wf %>% 
  tune_grid(
    resamples = cv_split,
    grid = penalty_grid,
)


lasso_tune %>% 
  select(id, .metrics) %>% 
  unnest(.metrics) %>% 
  filter(.metric == "accuracy")


```


```{r}
lasso_tune %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  scale_x_log10(
   breaks = scales::trans_breaks("log10", function(x) 10^x),
   labels = scales::trans_format("log10",scales::math_format(10^.x))) +
  labs(x = "penalty", y = "accuracy")
```


```{r}
lasso_tune %>% 
  show_best(metric = "accuracy")
```

```{r}
best_param <- lasso_tune %>% 
  select_best(metric = "accuracy")
best_param
```


```{r}
best_param <- lasso_tune %>%
  select_best(metric = "accuracy")

best_param
```


```{r}
lasso_final_wf <- lasso_wf %>% 
  finalize_workflow(best_param)
lasso_final_wf
```



```{r}

x <- model.matrix(score ~ .)

lasso_final_mod <- lasso_final_wf %>% 
  fit(data = drive_two_min_training)

lasso_final_mod %>% 
  pull_workflow_fit() %>% 
  tidy() 
```


```{r}
lasso_final_mod %>% 
  pull_workflow_fit() %>% 
  vip()
```


```{r}
lasso_test <- lasso_final_wf %>% 
  last_fit(cv_split)

lasso_test %>% 
  collect_metrics()
```

```{r}
preds <-
  collect_predictions(lasso_test) 
conf_mat(preds, .pred_class, score)
```


```{r}
lasso_preds <- collect_predictions(lasso_tune)

```

```{r}
conf_mat(lasso_preds, truth = score,
         estimate = .pred_class)
```

```{r}
accuracy(lasso_preds, truth = score,
         estimate = .pred_class)

(2541+978)/(2541+978+703+430)
```
```{r}
sens(lasso_preds, truth = score,
         estimate = .pred_class)


```
**Ratio between how much classified as score to how much was actually a score**

```{r}
spec(lasso_preds, truth = score,
         estimate = .pred_class)
```
**Ratio between how much classified as no score to how much was actually no score**

```{r}
precision(lasso_preds, truth = score,
         estimate = .pred_class)
```
**How much were correctly classified as a score out of all scores?**


```{r}
f_meas(lasso_preds, truth = score,
         estimate = .pred_class)
```
**The F1 score is about 0.818, which indicates that the trained model has a classification strength of 81.8%%.**


```{r}
custom_metrics <- metric_set(accuracy, sens, spec, precision, f_meas)

custom_metrics(lasso_preds, truth = score,
         estimate = .pred_class)
```


```{r}
roc_auc(lasso_preds,
        truth = score,
        .pred_1)
```

**ROC-AUC is a performance measurement for the classification problem at various thresholds settings. ROC_AUC tells how much the model is capable of distinguishing between classes. The trained logistic regression model has a ROC-AUC of ______**

```{r}
lasso_preds %>%
  roc_curve(truth = score, .pred_1) %>%
  autoplot()
```


```{r}
lasso_preds %>%
  ggplot() +
  geom_density(aes(x = .pred_1, fill = score), 
               alpha = 0.5)
```
```{r}
test_predictions <- rf_fit %>% pull(.predictions)
test_predictions
```



```{r}
lasso_final_wf <- lasso_wf %>% 
  finalize_workflow(best_param)
lasso_final_wf
```


```{r}
lasso_final_mod <- lasso_final_wf %>% 
  fit(data = drive_two_min_training)

lasso_final_mod %>% 
  pull_workflow_fit() %>% 
  tidy() 
```

```{r}
new_qb <- tribble(~pregnant, ~glucose, ~pressure, ~triceps, ~insulin, ~mass, ~pedigree, ~age,
                     2, 95, 70, 31, 102, 28.2, 0.67, 47)
new_qb
```

```{r}
predict(lasso_final_wf, new_data = new_qb)

```

```{r}
lasso_obj <- pull_workflow_fit(lasso_final_wf)$fit
```



- look at predicted probabilities of scoring vs if they scored or not -- density plot usually but because so few points, need another approach ... say a table?

- try different types of models if predicts majority class...

- try decision tree??? 


# Decision Tree

```{r}
library(tidymodels) 
library(tidyverse) # manipulating data
library(skimr) # data visualization
library(baguette) # bagged trees
library(future) # parallel processing & decrease computation time
library(xgboost) # boosted trees
```

```{r}

set.seed(2)

drive_two_min_split <- initial_split(drive_summary_data, 
                             prop = .75, strata = score)

drive_two_min_training <- training(drive_two_min_split)
drive_two_min_testing <- testing(drive_two_min_split)

cv_split <- vfold_cv(drive_two_min_training, 
                              v = 5)
```


```{r}
rf_recipe <- recipe(score ~ ., 
                       data = drive_two_min_training) %>% 
  step_upsample(score, over_ratio = 1) %>%
  step_dummy(all_nominal(), 
             -all_outcomes()) %>%
  step_normalize(all_predictors(), 
                 -all_outcomes())
```


```{r}
set.seed(2)
mod_rf <-rand_forest() %>%
  set_engine("ranger",
             num.threads = parallel::detectCores(), 
             importance = "permutation", 
             verbose = TRUE) %>% 
  set_mode("classification") %>% 
  set_args(trees = 1000)
```

```{r}
wflow_rf <- workflow() %>% 
  add_model(mod_rf) %>% 
  add_recipe(rf_recipe)
```



```{r}
set.seed(2)
plan(multisession)

fit_rf <- fit_resamples(
  wflow_rf,
  cv_split,
  metrics = metric_set(accuracy),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) x)
)
```

