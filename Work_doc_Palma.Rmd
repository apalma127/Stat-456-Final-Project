---
title: "Work Doc - Palma"
author: "Palma"
date: "11/16/2021"
output: html_document
---


For the projects, I want to give some suggested due dates:

Friday 11/19: 3. Outline of analysis plan with some of the steps completed.

Tuesday 11/23: 1. Read in and clean other two sources of data. 2. Finish a couple of the analysis steps and write them up completely. I recommend focusing on the "final product" first. 3. Create an RMarkdown file for the "behind-the-scenes" report and be sure to keep track of any detailed explanations of methods and code there. 

Thursday 12/2: 1. Complete most of the analyses and have a draft of the "final product". 2. Begin creating a presentation. 3. Create a detailed outline of what will go in the "behind-the-scenes". 

Tuesday 12/7: 1. "Final product" complete. 2. Finish the 15 minute presentation and practice it with your group. 

Thursday 12/9: 1. Submit "final product" on moodle. 2. Give presentations - we may need to use a bit of the lunch hour. 

Thursday 12/16: Submit "behind-the-scenes" (you can always submit early, if you'd like). 


```{r}
library(nflfastR)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggrepel)
library(stringr)
library(lubridate)

library(tidymodels)        # for modeling
library(themis)            # for step functions for unbalanced data
library(doParallel)        # for parallel processing
library(stacks)            # for stacking models
library(naniar)            # for examining missing values (NAs)
library(moderndive)        # for King County housing data
library(vip)               # for variable importance plots
library(patchwork)         # for combining plots nicely
library(ranger)
library(xgboost)
library(dplyr)
```


```{r}
pbp_2021 <- load_pbp(2021)
nfl_qbr_weekly <- readr::read_csv("https://raw.githubusercontent.com/nflverse/espnscrapeR-data/master/data/qbr-nfl-weekly.csv")
nfl_qbr_weekly<-nfl_qbr_weekly %>% 
  filter(season==2021)
```


```{r}
#devtools::install_github(repo = "ryurko/nflscrapR")
```


```{r}
library(nflscrapR)
library(na.tools)

```

```{r, eval=FALSE}
Two_min_drill <- pbp_2021 %>% 
  filter(half_seconds_remaining<120, as.numeric(ms(drive_game_clock_start))<150) 
  
Two_min_drill %>% 
  group_by(game_id, drive) %>%
  select(game_id, drive_play_count, ydsnet, drive_game_clock_start, fixed_drive_result, name, posteam, week, drive_start_yard_line) %>%
  filter(game_id == "2021_01_ARI_TEN", drive==11) %>% 
  mutate(td = ifelse(fixed_drive_result=='Touchdown', 1, 0),
  fg = ifelse(fixed_drive_result=='Field goal', 1, 0)) %>%
  #mutate(yrds_to_go_start = ifelse(str_sub(drive_start_yard_line, 1, (str_locate(drive_start_yard_line, " ")-1))== posteam, 100-as.numeric(str_sub(drive_start_yard_line, (str_locate(drive_start_yard_line, " ")+1), length(drive_start_yard_line))), as.numeric(str_sub(drive_start_yard_line, (str_locate(drive_start_yard_line, " ")+1), length(drive_start_yard_line))))) %>% 
  mutate(score= ifelse(td+fg==1, 1,0)) %>% 
  right_join(nfl_qbr_weekly, by = c("week" = "game_week", "posteam"="team_abb")) %>% 
  select(!game_id.y)
  # ggplot(aes(x= score, y=qbr_total))+
  # geom_boxplot()+
  # facet_wrap(vars(score))
```


```{r}
Two_min_drill

Two_min_drill %>% 
  group_by(game_id, drive) %>%
  mutate(rush_attempt = ifelse(is.na(rush_attempt), 0, rush_attempt)) %>% 
  mutate(pass_attempt = ifelse(is.na(pass_attempt), 0, pass_attempt)) %>%
  summarize(num_plays= n(), num_rush=sum(rush_attempt), num_pass= sum(pass_attempt))
  
```


```{r}
two_min_drill <- pbp_2021 %>% 
  filter(half_seconds_remaining<120, as.numeric(ms(drive_game_clock_start))<150) 
  
two_min_new <- two_min_drill %>% 
  group_by(game_id, drive) %>%
  mutate(td = ifelse(fixed_drive_result=='Touchdown', 1, 0),
  fg = ifelse(fixed_drive_result=='Field goal', 1, 0), 
  score= ifelse(td+fg==1, 1,0)) %>%
  right_join(nfl_qbr_weekly, by = c("week" = "game_week", "posteam"="team_abb")) %>% 
  ungroup()

```

```{r}
getmode <- function(v) {
    v <- na.rm()
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
```


```{r}
two_min_new <- two_min_new[!is.na(two_min_new$passer), ] 
```

```{r}
two_min_new %>%
  select(passer, ydsnet, qbr_total, qbr_raw, game_id.x)
```


```{r}
two_min_test <- 
  two_min_new  %>%
    group_by(drive, game_id.x) %>%
    mutate(run_plays = sum(rush_attempt, na.rm = TRUE), 
           pass_plays = sum(pass_attempt, na.rm = TRUE), 
           pass_tot_yds = sum(air_yards, na.rm = TRUE), 
           completion_perc = (1- sum(incomplete_pass, na.rm = TRUE) / pass_plays), 
           #penalties = sum(penalty, na.rm = TRUE), 
           tot_yds = sum(yards_gained, na.rm = TRUE),
           rush_yds_tot = sum(rushing_yards, na.rm = TRUE)) %>% 
           #rusher = as.character(rusher),
           #rusher = ifelse( is.na(rusher) == TRUE, getmode(rusher), rusher)) %>% 
     mutate(td = ifelse(fixed_drive_result=='Touchdown', 1, 0),
            fg = ifelse(fixed_drive_result=='Field goal', 1, 0), 
            score = ifelse(td+fg==1, 1,0)) %>%
    select(passer, qbr_raw, qbr_total, pass_tot_yds, tot_yds, ydsnet, rush_yds_tot, completion_perc, run_plays, pass_plays, drive_yards_penalized, tot_yds, drive_game_clock_start, td, fg, score, posteam, drive_start_yard_line)
```


```{r}
two_min_by_drive <-
  two_min_test %>%
    mutate(yards_to_go_start= ifelse(str_extract(drive_start_yard_line, "[A-Z]+")== posteam, 100- parse_number(drive_start_yard_line), parse_number(drive_start_yard_line))) 
  

two_min_by_drive
```

```{r}
drive_summary_data <- two_min_by_drive %>%
 arrange(game_id.x, drive) %>% 
 group_by(game_id.x) %>% 
  mutate(
    td = as.factor(td), 
    fg = as.factor(fg), 
    score = as.factor(score),
    drive_game_clock_start = as.numeric(ms(drive_game_clock_start))
    ) %>%
 summarise_all(last)

drive_summary_data
```

```{r}
drive_summary_data %>%
  select(game_id.x, passer, drive_game_clock_start, completion_perc, yards_to_go_start)
```

* need third dataset

* best of drives and total yards



 -- logistic regression as primary -- predicting PERCENT OF SCORE
 
 -- more lasso / rf / xg boost -- predicting SCORE using full model -- stacking? 
 
 -- sep rmd for visualizations and modeling
 
 * time left in half / game 
 
 * Model = predictive 
 
  -- two min drill w 40 sec w all these plays, what is likelihood of scoring????
  




## Exploratory Work


1.  Exploring Data

**Intro Visualizations**


```{r}
# qbr // score

drive_summary_data %>%  
  ggplot(aes(x = qbr_total, y = score)) +
  geom_boxplot()

drive_summary_data %>%  
  ggplot(aes(x = qbr_total)) +
  geom_histogram(bins = 40) +
  geom_vline(data = filter(drive_summary_data, score=="0"), aes(xintercept= mean(qbr_total)), colour="blue") +
  geom_vline(data = filter(drive_summary_data, score=="1"), aes(xintercept= mean(qbr_total)), colour="blue") +
  facet_wrap(~score) 

drive_summary_data %>%  
  ggplot(aes(x = qbr_total)) +
  geom_freqpoly(bins = 40) + 
  geom_vline(data = filter(drive_summary_data, score=="0"), aes(xintercept= mean(qbr_total)), colour="blue") +
  geom_vline(data = filter(drive_summary_data, score=="1"), aes(xintercept= mean(qbr_total)), colour="blue") +
  facet_wrap(~score) 


```



```{r}
# qbr // compl perc

drive_summary_data %>%
   ggplot(aes(x = completion_perc)) +
   geom_histogram(bins = 15)

 drive_summary_data %>%
   ggplot(aes(x = qbr_total, y = completion_perc)) +
   geom_point()

 drive_summary_data %>%
   ggplot(aes(x = qbr_total, y = completion_perc)) +
   geom_smooth()

q <- seq(0.1, 0.9, by = 0.1)

drive_summary_data %>%  
  ggplot(aes(x = qbr_total, y = completion_perc)) +
  geom_quantile(quantiles = q)
```



```{r}
# tot yds in drive (x) // amt of time left (ms drive gamenstart from ben code) (y) , score as color or facet

# game time var = drive_game_clock_start

drive_summary_data %>%
   ggplot(aes(x = tot_yds)) +
   geom_histogram(bins = 30)

drive_summary_data %>%
   ggplot(aes(x = drive_game_clock_start)) +
   geom_histogram(bins = 30)

 drive_summary_data %>%
   ggplot(aes(y = tot_yds, x = drive_game_clock_start)) +
   geom_point()

 drive_summary_data %>%
   ggplot(aes(x = qbr_total, y = completion_perc)) +
   geom_smooth()

q <- seq(0.1, 0.9, by = 0.1)

drive_summary_data %>%  
  ggplot(aes(x = qbr_total, y = completion_perc)) +
  geom_quantile(quantiles = q)


```



**Quant Vars**

```{r}
drive_summary_data %>% 
  select(where(is.numeric)) %>% 
  pivot_longer(cols = everything(),
               names_to = "variable", 
               values_to = "value") %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30) +
  facet_wrap(vars(variable), 
             scales = "free", 
            nrow = 3)

```


# Lasso Logisitic Regression 

```{r}
library(dplyr)       # for data manipulation (dplyr) 
library(broom)       # for making model summary tidy
library(visreg)      # for plotting logodds and probability 
library(margins)     # to calculate Average Marginal Effects
library(ROCR)  
```

```{r}
drive_summary_data$completion_perc[is.na(drive_summary_data$completion_perc)] <- 0
```


```{r}
drive_summary_data <- 
  drive_summary_data %>%
  select( -posteam, -drive_start_yard_line, -td, -fg)
  #select(-game_id.x, -posteam, -drive_start_yard_line, -td, -fg)

```



```{r}

set.seed(2)

drive_two_min_split <- initial_split(drive_summary_data, 
                             prop = .75, strata = score)

drive_two_min_training <- training(drive_two_min_split)
drive_two_min_testing <- testing(drive_two_min_split)


```


3. Set up the recipe and the pre-processing steps to build a lasso model



```{r}
set.seed(2)

lasso_recipe <- recipe(score ~ ., 
                       data = drive_two_min_training) %>% 
  step_upsample(score, over_ratio = 1) %>%
  step_dummy(all_nominal(), 
             -all_outcomes()) %>%
  step_normalize(all_predictors(), 
                 -all_outcomes())

```


```{r}
lasso_recipe %>% 
  prep(drive_two_min_training) %>%
  juice() 
```

```{r}
table(drive_two_min_training$score, useNA = "always")
```



# Lasso Model and WF

```{r}
lasso_mod  <- 
  logistic_reg(mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_args(penalty = tune()) %>% 
  set_mode("classification")

lasso_wf <-  workflow() %>% 
  add_recipe(lasso_recipe) %>% 
  add_model(lasso_mod)

```

# More Lasso Set up

```{r}

set.seed(2) #for reproducible 5-fold
val_split <- validation_split(drive_two_min_training, 
                              prop = .6)
val_split

penalty_grid <- grid_regular(penalty(),
                             levels = 10)

ctrl_grid <- control_stack_grid()

lasso_tune <-  
  lasso_wf %>% 
  tune_grid(
    resamples = val_split,
    grid = penalty_grid,
    control = ctrl_grid)

lasso_tune %>% 
  select(id, .metrics) %>% 
  unnest(.metrics) %>% 
  filter(.metric == "accuracy")
```


- **ADD MORE YEARS...** -- 2018 - 2021 -- just validation -- 1k points or more for cv 

- try other methods 

- use historical data and leave out 2020 as TESTING!

- such small data set, take time investigating where predicting well....


- residual plot to investigate and make sure not predicting majority class

- check and make sure success isnn't already in set

- look at predicted probabilities of scoring vs if they scored or not -- density plot usually but because so few points, need another approach ... say a table?

- try different types of models if predicts majority class...

- try decision tree??? 






```{r}
lasso_tune %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  scale_x_log10(
   breaks = scales::trans_breaks("log10", function(x) 10^x),
   labels = scales::trans_format("log10",scales::math_format(10^.x))) +
  labs(x = "penalty", y = "accuracy")
```


```{r}
lasso_tune %>% 
  show_best(metric = "accuracy")
```

```{r}
best_param <- lasso_tune %>% 
  select_best(metric = "accuracy")
best_param
```


```{r}
lasso_tune %>%
  select_best(metric = "accuracy")
```



```{r}
rf_tune %>%
  select_best(metric = "accuracy")
```

